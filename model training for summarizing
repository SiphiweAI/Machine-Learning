# Imports
from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.model_selection import train_test_split
import pandas as pd

""" to prepare data and Tokenize """
# Load and prepare data
def load_and_prepare_data(file_path):
    data = pd.read_csv(file_path)                                                   # load data

    # Split dataset into train and validation sets
    def split_dataset(data, test_size=0.1):
        train_data, val_data = train_test_split(data, test_size=test_size, stratify=data['label'])
        return Dataset.from_pandas(train_data), Dataset.from_pandas(val_data)

    train_dataset, val_dataset = split_dataset(data)                                        # assigns values to the two variables by calling the function passing 'data' as arg
    
    return train_dataset, val_dataset

def tokenize_function(examples):                                                          
    inputs = tokenizer(examples['text'], max_length=1024, truncation=True, padding='max_length')
    targets = tokenizer(examples['summary'], max_length=150, truncation=True, padding='max_length')
    inputs['labels'] = targets['input_ids']
    return inputs


tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')                        # load tokenizer
train_dataset, val_dataset = load_and_prepare_data('Stories_and_summary.csv')               # load data

# Tokenize datasets
tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_val = val_dataset.map(tokenize_function, batched=True)


""" define Model and Training args"""

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')             # load model

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',                                                                  # directory for storing logs
    logging_steps=10,                                                                      # log every 10 steps
)

# Define Trainer
trainer = Trainer(
    model=model,                                                                            # model loaded above
    args=training_args,
    train_dataset=tokenized_train,                                                          # tokenized dataset, defined above
    eval_dataset=tokenized_val                                                              # tokenized dataset, defined above
)


""" train the model"""

# Train the model
trainer.train()

# Save the model
model.save_pretrained('./summary-trained-bart')                                             # can be used later 
